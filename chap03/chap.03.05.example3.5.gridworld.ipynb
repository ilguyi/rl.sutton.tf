{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3.5: Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/chap.03.05.example3.5.gridworld.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 cell에서 취할 수 있는 actions: `north`, `south`, `east`, and `west`\n",
    "* deterministic action\n",
    "* 경계를 벗어나는 action에 대해서는 state가 변하지 않고 reward `-1`을 받음\n",
    "* 나머지 다른 action(state `A`와 `B`에 있을때를 빼고)에 대해서는 reward `0`을 받음\n",
    "* state `A`에서는 어떤 action을 하던지 state `A'`으로 가고 reward `+10`을 받음\n",
    "* state `B`에서는 어떤 action을 하던지 state `B'`으로 가고 reward `+5`을 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid world state index\n",
    "\n",
    "|   |   |   |   |   |\n",
    "|----|----|----|----|----|\n",
    "| 0,0  | 0,1 | 0,2 | 0,3 | 0,4 |\n",
    "| 1,0  | 1,1 | 1,2 | 1,3 | 1,4 |\n",
    "| 2,0  | 2,1 | 2,2 | 2,3 | 2,4 |\n",
    "| 3,0  | 3,1 | 3,2 | 3,3 | 3,4 |\n",
    "| 4,0  | 4,1 | 4,2 | 4,3 | 4,4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman equation\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a} \\pi(a | s)\n",
    "\\sum_{s', r} p(s', r | s, a)\n",
    "\\left[ r + \\gamma v_{\\pi}(s') \\right]$$\n",
    "\n",
    "* $p(s', r | s, a)$: deterministic\n",
    "  * $p(s'=(4,1), \\, r=10 \\ | \\ s=(0,1), \\, a=\\textrm{'north'}) = 1$\n",
    "    * state $A$에서 'north' 방향으로 움직였을 때 다음 state가 $(4,1)$ 이고 reward 10을 받을 확률은 1\n",
    "  * $p(s'=(2,1), \\, r=0 \\ | \\ s=(1,1), \\, a=\\textrm{'north'}) = 0$\n",
    "    * state $(1,1)$에서 'north' 방향으로 움직였을 때 다음 state가 $(2,1)$ 이고 reward 0을 받을 확률은 0\n",
    "    * deterministic 이라 $s'=(0,1)$만 허용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `self.M`\n",
    "\n",
    "* linear equation의 계수를을 모아놓은 matrix\n",
    "\n",
    "$$ M V = R$$\n",
    "\n",
    "$$ \\left[ \\begin{array}{cccc}\n",
    "w_{0,0} & w_{0,1} & \\cdots  & w_{0,24} \\\\\n",
    "w_{1,0} & w_{1,1} & \\cdots  & w_{1,24} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "w_{24,0} & w_{24,1} & \\cdots  & w_{24,24}\n",
    "\\end{array} \\right]\n",
    "\\left[ \\begin{array}{c}\n",
    "v_{\\pi}(s_{(0, 0)}) \\\\\n",
    "v_{\\pi}(s_{(0, 1)}) \\\\\n",
    "\\vdots \\\\\n",
    "v_{\\pi}(s_{(4, 4)}) \\\\\n",
    "\\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c}\n",
    "\\frac{1}{4} R_{s_{(0, 0)}} \\\\\n",
    "\\frac{1}{4} R_{s_{(0, 1)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{1}{4} R_{s_{(4, 4)}} \\\\\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "where $R_{s_{(0, 0)}} = r_{a=\\textrm{'north'}} + r_{a=\\textrm{'south'}} + r_{a=\\textrm{'east'}} + r_{a=\\textrm{'west'}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "  \n",
    "  def __init__(self, L: int = 5) -> None:\n",
    "    self.L = L\n",
    "    self.N = self.L * self.L\n",
    "    self.actions_list = ['north', 'south', 'east', 'west']\n",
    "    self.rewards_list = [10, 5, 0, -1]\n",
    "    self.p = np.zeros((self.N, len(self.rewards_list),\n",
    "                       self.N, len(self.actions_list)))\n",
    "    \n",
    "    # set state2ij and ij2state\n",
    "    self.state2ij = np.zeros(shape=(self.N, 2), dtype=np.int64)\n",
    "    self.ij2state = {}\n",
    "    for i in range(self.L):\n",
    "      for j in range(self.L):\n",
    "        self.state2ij[i * self.L + j] = (i, j)\n",
    "        self.ij2state[(i, j)] = i * self.L + j\n",
    "        \n",
    "    self.A = (0, 1)  # special site\n",
    "    self.B = (0, 3)  # special site\n",
    "    self.A_ = (4, 1)  # special site\n",
    "    self.B_ = (2, 3)  # special site\n",
    "    \n",
    "    # assign transition matrix p describing model dynamics\n",
    "    for s in range(self.N):\n",
    "      if self.is_A_or_B(s):\n",
    "        continue\n",
    "      for a_idx, a in enumerate(self.actions_list):\n",
    "        for s_ in range(self.N):\n",
    "          for r_idx, r in enumerate(self.rewards_list):\n",
    "            expected_s, expected_r = self.step(s, a)\n",
    "\n",
    "            if expected_s == s_ and expected_r == r:\n",
    "              # print(s_, r_idx, s, a_idx)\n",
    "              self.p[s_, r_idx, s, a_idx] = 1.\n",
    "        \n",
    "  def north(self, i: int, j: int) -> typing.Tuple[typing.Tuple[int, int], bool]:\n",
    "    if i == 0:\n",
    "      return (i, j), False\n",
    "    else:\n",
    "      return (i - 1, j), True\n",
    "\n",
    "  def south(self, i: int, j: int) -> typing.Tuple[typing.Tuple[int, int], bool]:\n",
    "    if i == self.L - 1:\n",
    "      return (i, j), False\n",
    "    else:\n",
    "      return (i + 1, j), True\n",
    "\n",
    "  def east(self, i: int, j: int) -> typing.Tuple[typing.Tuple[int, int], bool]:\n",
    "    if j == self.L - 1:\n",
    "      return (i, j), False\n",
    "    else:\n",
    "      return (i, j + 1), True\n",
    "\n",
    "  def west(self, i: int, j: int) -> typing.Tuple[typing.Tuple[int, int], bool]:\n",
    "    if j == 0:\n",
    "      return (i, j), False\n",
    "    else:\n",
    "      return (i, j - 1), True\n",
    "  \n",
    "  def transition(self, state: int, action: str) -> typing.Tuple[int, bool]:\n",
    "    i, j = self.state2ij[state]\n",
    "    if action == 'north':\n",
    "      new_state, is_moving = self.north(i, j)\n",
    "    elif action == 'south':\n",
    "      new_state, is_moving = self.south(i, j)\n",
    "    elif action == 'east':\n",
    "      new_state, is_moving = self.east(i, j)\n",
    "    elif action == 'west':\n",
    "      new_state, is_moving = self.west(i, j)\n",
    "\n",
    "    return self.ij2state[new_state], is_moving\n",
    "  \n",
    "  def is_A_or_B(self, state: int) -> bool:\n",
    "    i, j = self.state2ij[state]\n",
    "    if self.A == (i, j):\n",
    "      state_A_ = self.ij2state[self.A_]\n",
    "      self.p[state_A_, 0, state, :] = 1.\n",
    "      return True\n",
    "    if self.B == (i, j):\n",
    "      state_B_ = self.ij2state[self.B_]\n",
    "      self.p[state_B_, 1, state, :] = 1.\n",
    "      return True\n",
    "    \n",
    "    return False\n",
    "  \n",
    "  def dynamics(self, state_: int, reward_idx: int,\n",
    "               state: int, action_idx: int) -> float:\n",
    "    return self.p[state_, reward_idx, state, action_idx]\n",
    "  \n",
    "  def step(self, state: int, action: str) -> typing.Tuple[int, int]:\n",
    "    assert action in self.actions_list\n",
    "    i, j = self.state2ij[state]\n",
    "    if self.A == (i, j):\n",
    "      return self.ij2state[self.A_], 10\n",
    "    if self.B == (i, j):\n",
    "      return self.ij2state[self.B_], 5\n",
    "\n",
    "    new_state, is_moving = self.transition(state, action)\n",
    "    if is_moving:\n",
    "      return new_state, 0\n",
    "    else:\n",
    "      return new_state, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "  \n",
    "  def __init__(self) -> None:\n",
    "    self.action_list = ['north', 'south', 'east', 'west']\n",
    "    \n",
    "  def policy(self, state: int, action: str) -> float:\n",
    "    return 0.25\n",
    "  \n",
    "  def action(self, state: int) -> str:\n",
    "    probabilities = np.array([self.policy(state, action) for action in self.action_list])\n",
    "    assert np.sum(probabilities) == 1\n",
    "    \n",
    "    return np.random.choice(self.action_list, p=probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GridWorld()\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear equations\n",
    "L = model.L\n",
    "N = model.N\n",
    "w = np.zeros((N, N))\n",
    "b = np.zeros(N)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(N):\n",
    "  for s_ in range(N):\n",
    "    for a_idx, a in enumerate(model.actions_list):\n",
    "      for r_idx, r in enumerate(model.rewards_list):\n",
    "        coeff = agent.policy(s, a) * model.dynamics(s_, r_idx, s, a_idx)\n",
    "        w[s, s_] += coeff * gamma\n",
    "        b[s] += coeff * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_matrix = np.eye(N)\n",
    "w = w - identity_matrix\n",
    "b = -b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.3  8.8  4.4  5.3  1.5]\n",
      " [ 1.5  3.   2.3  1.9  0.5]\n",
      " [ 0.1  0.7  0.7  0.4 -0.4]\n",
      " [-1.  -0.4 -0.4 -0.6 -1.2]\n",
      " [-1.9 -1.3 -1.2 -1.4 -2. ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.linalg.solve(w, b)\n",
    "x = x.reshape(L, L)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "<img src=\"figures/chap.03.05.example3.5.gridworld.png\" width=\"70%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
