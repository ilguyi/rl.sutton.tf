{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/chap.04.01.example1.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "  def __init__(self, size=4, terminal_states=[(0, 0), (3, 3)]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      size: int, Grid World size\n",
    "      terminal_states: list of tuples\n",
    "    \"\"\"\n",
    "    self.actions = ['up', 'down', 'right', 'left']\n",
    "    self.rewards = [-1, 0]\n",
    "    self.terminal_states = terminal_states # special state (terminal state)\n",
    "    self.values = np.zeros((size, size))\n",
    "    #self.values = np.random.normal(scale=0.1, size=(size, size))\n",
    "    #self.values[0, 0] = 0.\n",
    "    #self.values[-1, -1] = 0.\n",
    "    self.gamma = 1.0\n",
    "    self.size = size\n",
    "    self.theta = 0.0001 # convergence precision\n",
    "    \n",
    "    \n",
    "  def Step(self, state, action):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      state: tuple (x, y) coordinate\n",
    "      action: string\n",
    "      \n",
    "    Returns:\n",
    "      next_state: tuple (x, y) coordinate\n",
    "      reward: int\n",
    "    \"\"\"\n",
    "    if state in self.terminal_states:\n",
    "      # terminal state에 있으면 모든 action에 next_state=state, reward=0 을 준다.\n",
    "      next_state = state\n",
    "      reward = 0\n",
    "    else:\n",
    "      if action == 'up':\n",
    "        if state[0] > 0:\n",
    "          next_state = (state[0]-1, state[1])\n",
    "          reward = -1\n",
    "        else:\n",
    "          next_state = state\n",
    "          reward = -1\n",
    "      elif action == 'down':\n",
    "        if state[0] < self.size-1:\n",
    "          next_state = (state[0]+1, state[1])\n",
    "          reward = -1\n",
    "        else:\n",
    "          next_state = state\n",
    "          reward = -1\n",
    "      elif action == 'right':\n",
    "        if state[1] < self.size-1:\n",
    "          next_state = (state[0], state[1]+1)\n",
    "          reward = -1\n",
    "        else:\n",
    "          next_state = state\n",
    "          reward = -1\n",
    "      elif action == 'left':\n",
    "        if state[1] > 0:\n",
    "          next_state = (state[0], state[1]-1)\n",
    "          reward = -1\n",
    "        else:\n",
    "          next_state = state\n",
    "          reward = -1\n",
    "    return next_state, reward\n",
    "  \n",
    "  def IterativePolicyEvaluation(self, policy):\n",
    "    iteration = 0\n",
    "    while True:\n",
    "      delta = 0\n",
    "      #print(iteration)\n",
    "      for i in range(self.size):\n",
    "        for j in range(self.size):\n",
    "          if (i==0 and j==0) or (i==self.size-1 and j==self.size-1):\n",
    "            continue\n",
    "          else:\n",
    "            v = self.values[i, j]\n",
    "            new_value = 0.\n",
    "            for key, value in policy.get_policy_at_state(state=(i, j)).items():\n",
    "              next_state, reward = self.Step(state=(i, j), action=key)\n",
    "              new_value += value * (reward + self.gamma * self.values[next_state[0], next_state[1]])\n",
    "            self.values[i, j] = new_value\n",
    "            delta = np.maximum(delta, np.abs(v - self.values[i, j]))\n",
    "      iteration += 1\n",
    "      if delta < self.theta:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "  def __init__(self, size=4):\n",
    "    self.init_actions = {'up': 0.25,\n",
    "                         'down': 0.25,\n",
    "                         'right': 0.25,\n",
    "                         'left': 0.25}\n",
    "    self.policy = np.asarray([self.init_actions] * size * size).reshape((size, size))\n",
    "    \n",
    "  def get_policy_at_state(self, state):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      state: tuple (x, y) coordinate\n",
    "    \"\"\"\n",
    "    return self.policy[state[0], state[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Policy()\n",
    "g = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.IterativePolicyEvaluation(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/chap.04.01.example2.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
